{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Author: Victor Diallen Andrade do Amaral","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents :\n* [1. Introduction](#section1)\n* [2. Importing Libraries](#section2)\n* [3. Loading Datasets](#section3)\n* [4. Data Analysis](#section4)\n* [5. Data Visualization](#section5)\n* [6. Machine Learning](#section6)\n* [7. Conclusion](#section7)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section1\"></a>\n# Introduction","metadata":{}},{"cell_type":"markdown","source":"## Kaggle Dataset Link","metadata":{}},{"cell_type":"markdown","source":"https://www.kaggle.com/datasets/krzysztofjamroz/apartment-prices-in-poland?select=apartments_pl_2023_10.csv","metadata":{}},{"cell_type":"markdown","source":"## About Dataset\n\nThe dataset contains apartment offers from the 15 largest cities in Poland (Warsaw, Lodz, Krakow, Wroclaw, Poznan, Gdansk, Szczecin, Bydgoszcz, Lublin, Katowice, Bialystok, Czestochowa). The data comes from local websites with apartments for sale. To fully capture the neighborhood of each apartment better, each offer was extended by data from the Open Street Map with distances to points of interest (POI). The data is collected monthly and covers timespan between September 2023 and October 2023","metadata":{}},{"cell_type":"markdown","source":"## Variables Description\n\n- **city** - the name of the city where the property is located\n- **type** - type of the building\n- **squareMeters** - the size of the apartment in square meters\n- **rooms** - number of rooms in the apartment\n- **floor** / floorCount - the floor where the apartment is located and the total number of floors in the building\n- **buildYear** - the year when the building was built\n- **latitude, longitude** - geo coordinate of the property\n- **centreDistance** - distance from the city centre in km\n- **poiCount** - number of points of interest in 500m range from the apartment (schools, clinics, post offices, kindergartens, -  - - restaurants, colleges, pharmacies)\n- **[poiName]Distance** - distance to the nearest point of interest (schools, clinics, post offices, kindergartens, restaurants, colleges, pharmacies)\n- **ownership** - the type of property ownership\n- **condition** - the condition of the apartment\n- **has[features]** - whether the property has key features such as assigned parking space, balcony, elevator, security, storage room\n- **price** - offer price in Polish Zloty","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section2\"></a>\n# Importing Libraries","metadata":{}},{"cell_type":"code","source":"# Python version used\nfrom platform import python_version\nprint('Python Version Used in this Jupyter Notebook:', python_version())","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import linear_model\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import make_pipeline\nfrom scipy.stats import skew\nfrom sklearn.linear_model import LinearRegression, Ridge, LassoCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn import linear_model\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import RepeatedKFold\nfrom numpy import absolute\nimport graphviz\nimport xgboost as xgb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section3\"></a>\n# Loading Datasets","metadata":{}},{"cell_type":"code","source":"df_august = pd.read_csv('/kaggle/input/apartment-prices-in-poland/apartments_pl_2023_08.csv')\ndf_september = pd.read_csv('/kaggle/input/apartment-prices-in-poland/apartments_pl_2023_09.csv')\ndf_october = pd.read_csv('/kaggle/input/apartment-prices-in-poland/apartments_pl_2023_10.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_august.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_september.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_october.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_august.shape)\nprint(df_september.shape)\nprint(df_october.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Concatenating Datasets","metadata":{}},{"cell_type":"code","source":"df_august['Month'] = 0\ndf_september['Month'] = 1\ndf_october['Month'] = 2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = [df_august, df_september, df_october]\ndf = pd.concat(frames)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section4\"></a>\n# Data Analysis","metadata":{}},{"cell_type":"code","source":"# Shape of dataframe\ndf.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for missing values\ndf.isna().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping id and columns which have a very high number of missing values, being impossible to apply techniques such as imputation\ndf.drop(['id','type', 'floor', 'buildYear', 'floorCount', 'condition', 'buildingMaterial'], axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Month'] = df['Month'].astype(str)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping remaining missing values\ndf_clean = df.dropna()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping duplicates if any\ndf_clean = df_clean.drop_duplicates().reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cell in case you want to save the new and clean dataframe to csv.\n# df_clean = pd.to_csv('df_all_clean.csv', index=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Selecting Numerical and Categorical Columns","metadata":{}},{"cell_type":"code","source":"num_cols = df_clean.select_dtypes([np.number]).columns\ndf_nums = df_clean[num_cols].reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols = df_clean.select_dtypes(['object']).columns\ndf_cats = df_clean[cat_cols].reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section5\"></a>\n# Data Visualization","metadata":{}},{"cell_type":"markdown","source":"## Univariate Analysis - Box Plots","metadata":{"id":"3S8mup8inZwG"}},{"cell_type":"code","source":"features = num_cols.to_list()\nplt.figure(figsize=(15,5))\nfor i in range(0, len(features)):\n    plt.subplot(2, 7, i + 1)\n    sns.boxplot(y = df_clean[features[i]], color = 'magenta', orient = 'v')\n    plt.tight_layout()","metadata":{"id":"OI0jseJSI3lH","outputId":"000242d3-5aa7-490d-916e-63778e6c9921","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- It's important to notice that although boxplot indicates outliers, some of them seem to be natural values and should leave them as it is.","metadata":{}},{"cell_type":"markdown","source":"## Univariate Analysis - Dist Plots","metadata":{"id":"y1TLfBFznpiI"}},{"cell_type":"code","source":"features = num_cols.to_list()\nplt.figure(figsize = (20, 10))\nfor i in range(0, len(features)):\n    plt.subplot(5, 3, i+1)\n    sns.histplot(x = df_clean[features[i]], kde = True, color = 'green')\n    plt.tight_layout()","metadata":{"id":"KAo4kMiYZkIx","outputId":"cb5f9f76-b9e5-4fec-f009-b24bb53b9ebe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Most columns seem to be skewed, which needs to be corrected later.","metadata":{}},{"cell_type":"markdown","source":"## Univariate Analysis - Violin Plots","metadata":{"id":"yFiXsWnlnxOT"}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nfeatures = num_cols.to_list()\nfor i in range(0, len(features)):\n    plt.subplot(3, 5, i+1)\n    sns.violinplot(y = df_clean[features[i]], color = 'yellow', orient = 'v')\n    plt.tight_layout()","metadata":{"id":"h2J1pRRKEIN6","outputId":"8b565499-76d5-433b-c9ec-5e9746f0403a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Univariate Analysis - Count Plot (Categorical)","metadata":{"id":"0CsxzknioPrX"}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nfor i in range(0, len(df_cats.columns)):\n    plt.subplot(4, 3, i+1)\n    ax = sns.countplot(y = df_clean[df_cats.columns[i]], palette = 'BuPu', orient = 'v')\n    ax.set_xlim(0,df_clean[df_cats.columns[i]].value_counts().max()+df_clean[df_cats.columns[i]].value_counts().max()*0.2)\n    ax.bar_label(ax.containers[0]);\n    plt.tight_layout()","metadata":{"id":"Htjc1m0NKjIh","outputId":"26d1a322-bd24-4b55-dc77-3eec20ebd3c3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Some of the categorical columns are way too imbalanced.","metadata":{}},{"cell_type":"markdown","source":"## Bivariate Analysis - Correlation Map","metadata":{"id":"a2qWEuSIoN_Z"}},{"cell_type":"code","source":"df_nums.corr()","metadata":{"id":"u5sKyU1cKr4O","outputId":"aee5b9e7-24fd-4fef-f501-65dbf8f84f50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_df = df_nums.corr()","metadata":{"id":"TNSjoidwLzk2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15, 8))\nsns.heatmap(corr_df, cmap = 'Blues', annot = True, fmt = '.2f')\nplt.xticks(rotation=45);","metadata":{"id":"4QUIr_NwL6F1","outputId":"8994a9ed-d807-4727-8c63-5c31b4c9982d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean[df_cats.columns] = df_clean[df_cats.columns].apply(LabelEncoder().fit_transform)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15, 8))\nsns.heatmap(df_clean.corr(), cmap = 'Blues', annot = True, fmt = '.2f')\nplt.xticks(rotation=45);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section6\"></a>\n# Machine Learning","metadata":{}},{"cell_type":"markdown","source":"## Feature Selection","metadata":{}},{"cell_type":"code","source":"# Dropping low correlation columns (interestingly, some of them have high multicollinearity)\ndf_clean.drop(['schoolDistance','latitude', 'postOfficeDistance',\n               'kindergartenDistance', 'collegeDistance', 'pharmacyDistance', 'hasBalcony','Month'], axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping rooms column because it has a high multicolinearity with squareMeters columns\ndf_clean.drop(['rooms'], axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Random Forest Classifier to Check the Most Important Features","metadata":{}},{"cell_type":"code","source":"X = df_clean.loc[:, df_clean.columns != 'price']\ny = df_clean['price'].values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=10, random_state=0, max_depth=9, n_jobs=-1)\n\nclf.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_scores = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n\nfeature_scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.barh(X.columns, clf.feature_importances_)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"markdown","source":"### Using X1","metadata":{}},{"cell_type":"code","source":"X1 = df_clean.loc[:, df_clean.columns != 'price']\ny1 = df_clean['price'].values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.25, random_state = 42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1_train_scaled = scaler.fit_transform(X1_train)\nX1_test_scaled = scaler.fit_transform(X1_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = linear_model.LinearRegression(fit_intercept = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_v1_lm = model.fit(X1_train_scaled, y1_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calcula a m√©trica R2 do nosso modelo\nr2_score(y1_test, model_v1_lm.fit(X1_train_scaled, y1_train).predict(X1_test_scaled))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using X2","metadata":{}},{"cell_type":"code","source":"X2 = df_clean[['squareMeters', 'longitude', 'centreDistance']]\ny2 = df_clean['price']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = 0.25, random_state = 42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X2_train_scaled = scaler.fit_transform(X2_train)\nX2_test_scaled = scaler.fit_transform(X2_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_v2_lm = model.fit(X2_train_scaled, y2_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the R2 metric\nr2_score(y2_test, model_v2_lm.fit(X2_train_scaled, y2_train).predict(X2_test_scaled))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- X2 had lower R2 score, but it's a way simpler model (containing less predictor variables), which makes it more generalizable ","metadata":{}},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"# Creating Random Forest Model\nrf1 = RandomForestRegressor(n_estimators = 100, min_samples_leaf = 10, random_state = 101, n_jobs=-1, max_depth=9, oob_score=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using X1","metadata":{}},{"cell_type":"code","source":"model_rf_v1 = rf1.fit(X1_train_scaled, y1_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model_rf_v1.predict(X1_test_scaled)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_squared_error(y1_test, prediction)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_absolute_error(y1_test, prediction)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r2_score(y1_test, prediction)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using X2","metadata":{}},{"cell_type":"code","source":"model_rf_v2 = rf1.fit(X2_train_scaled, y2_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model_rf_v2.predict(X2_test_scaled)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_squared_error(y2_test, prediction)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_absolute_error(y2_test, prediction)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r2_score(y1_test, prediction)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- X1 and X2 had similar performances. X2 being a simpler model, it would be chosen.","metadata":{}},{"cell_type":"markdown","source":"### Finding the best n_estimator","metadata":{}},{"cell_type":"code","source":"N_estimators = [5,50,100,200,500,1000]\nR2_score = []\nfor n_estimator in N_estimators:\n    model = RandomForestRegressor(n_estimators = n_estimator,max_depth = 9)\n    model.fit(X2_train_scaled, y2_train)\n    prediction = model.predict(X2_test_scaled)\n    r2_calc = r2_score(y2_test, prediction)\n    R2_score.append(r2_calc)\n    print(f'For {n_estimator} n_estimator and the R2 score is: ', r2_calc)\n","metadata":{"execution":{"iopub.execute_input":"2022-12-29T15:32:48.215867Z","iopub.status.busy":"2022-12-29T15:32:48.215412Z","iopub.status.idle":"2022-12-29T15:39:27.107689Z","shell.execute_reply":"2022-12-29T15:39:27.10584Z","shell.execute_reply.started":"2022-12-29T15:32:48.215839Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.plot(N_estimators, R2_score,c='g')\nfor i, txt in enumerate(np.round(R2_score,5)):\n    ax.annotate((N_estimators[i],str(txt)), (N_estimators[i],R2_score[i]))\nplt.grid()\nplt.title(\"R2 Score for each Estimator\")\nplt.xlabel(\"Estimator i's\")\nplt.ylabel(\"Score measure\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using GridSearchCV for Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"#param_grid = { \n#    'n_estimators': [25, 50, 100, 150, 200], \n#    'max_features': ['sqrt', 'log2', None], \n#    'max_depth': [3, 6, 9], \n#    'max_leaf_nodes': [3, 6, 9], \n#}\n\n\n#grid_search = GridSearchCV(RandomForestRegressor(), \n#                           param_grid=param_grid) \n#grid_search.fit(X1_train_scaled, y1_train) \n#print(grid_search.best_estimator_) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model_grid = RandomForestRegressor(max_depth=9,\n#                                   max_features=1.0,\n#                                   max_leaf_nodes=None,\n#                                   n_estimators=200,\n#                                   random_state=10) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model_grid.fit(X2_train_scaled, y2_train) \n#y_pred_grid = model_grid.predict(X2_test_scaled) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#r2_score(y2_test, y_pred_grid)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBRegressor","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(n_estimators=200, max_depth=6, eval_metric=[\"auc\", \"error\", \"error@0.6\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X2_train_scaled, y2_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict(X2_test_scaled)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(r2_score(y2_test, prediction))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(mean_squared_error(y2_test, prediction))\n\nprint(mean_absolute_error(y2_test, prediction))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section7\"></a>\n# Conclusion","metadata":{}},{"cell_type":"markdown","source":"- X2 selected features not always returned better evaluation metrics, but being a way simpler model, it should be chosen since its more generalizable.\n- Among all the algorithms XGBoost Regressor had better results.\n- Tuning hyperparameters might return better results.","metadata":{}}]}